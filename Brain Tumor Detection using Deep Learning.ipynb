{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brazilian-economics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Requirement already satisfied: numpy==1.20.3 in /opt/anaconda3/envs/Project/lib/python3.8/site-packages (1.20.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9abf8eb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "this version of pandas is incompatible with numpy < 1.20.3\nyour numpy version is 1.20.0.\nPlease upgrade numpy to >= 1.20.3 to use this pandas version",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms, models\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Project/lib/python3.8/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Project/lib/python3.8/site-packages/pandas/compat/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     is_numpy_dev,\n\u001b[1;32m     20\u001b[0m     np_version_under1p21,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[1;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Project/lib/python3.8/site-packages/pandas/compat/numpy/__init__.py:23\u001b[0m\n\u001b[1;32m     19\u001b[0m     np_percentile_argname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterpolation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _nlv \u001b[38;5;241m<\u001b[39m Version(_min_numpy_ver):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis version of pandas is incompatible with numpy < \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_min_numpy_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour numpy version is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_np_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease upgrade numpy to >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_min_numpy_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to use this pandas version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     30\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_np_version\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_numpy_dev\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m ]\n",
      "\u001b[0;31mImportError\u001b[0m: this version of pandas is incompatible with numpy < 1.20.3\nyour numpy version is 1.20.0.\nPlease upgrade numpy to >= 1.20.3 to use this pandas version"
     ]
    }
   ],
   "source": [
    "#Importing packages\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils.np_utils import to_categorical # used for converting labels to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "import itertools\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from keras.layers.core import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D,AveragePooling2D, Flatten, Dropout, Input, BatchNormalization\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3088868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting numpy==1.20\n",
      "  Downloading numpy-1.20.0-cp38-cp38-macosx_10_9_x86_64.whl.metadata (2.0 kB)\n",
      "Downloading numpy-1.20.0-cp38-cp38-macosx_10_9_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m234.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.3\n",
      "    Uninstalling numpy-1.20.3:\n",
      "      Successfully uninstalled numpy-1.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "rfpimp 1.3.2 requires sklearn, which is not installed.\n",
      "wrf-python 1.3.4.1 requires basemap, which is not installed.\n",
      "altair 5.2.0 requires typing-extensions>=4.0.1; python_version < \"3.11\", but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "bokeh 2.4.3 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "pandas 1.5.3 requires numpy>=1.20.3, but you have numpy 1.20.0 which is incompatible.\n",
      "pingouin 0.5.2 requires scikit-learn<1.1.0, but you have scikit-learn 1.1.3 which is incompatible.\n",
      "pyportfolioopt 1.5.5 requires numpy<2.0.0,>=1.22.4, but you have numpy 1.20.0 which is incompatible.\n",
      "sktime 0.14.0 requires numpy<1.23,>=1.21.0, but you have numpy 1.20.0 which is incompatible.\n",
      "tensorflow 2.4.1 requires numpy~=1.19.2, but you have numpy 1.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.20.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spiritual-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-effort",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "identical-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/kipkemoivincent/Desktop/Covid/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "directed-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 100\n",
    "IMG_HEIGHT = 100\n",
    "BATCH_SIZE = 2481"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-exchange",
   "metadata": {},
   "source": [
    "horizontal_flip=True, \n",
    "                                                               vertical_flip=True,zoom_range=0.3,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "correct-worse",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m      2\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(path,\n\u001b[1;32m      3\u001b[0m                                                    target_size\u001b[38;5;241m=\u001b[39m(IMG_WIDTH, IMG_HEIGHT),\n\u001b[1;32m      4\u001b[0m                                                    batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m      5\u001b[0m                                                    class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                                    shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m                                                    subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "train_datagen =tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255,validation_split=0.0001)\n",
    "train_generator = train_datagen.flow_from_directory(path,\n",
    "                                                   target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   class_mode='categorical',\n",
    "                                                   shuffle=True,\n",
    "                                                   subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reliable-taylor",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_datagen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241m.\u001b[39mflow_from_directory(path,\n\u001b[1;32m      2\u001b[0m                                                    target_size\u001b[38;5;241m=\u001b[39m(IMG_WIDTH, IMG_HEIGHT),\n\u001b[1;32m      3\u001b[0m                                                    batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m743\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                                    class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                                    shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                                                    subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_datagen' is not defined"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_directory(path,\n",
    "                                                   target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "                                                   batch_size=743,\n",
    "                                                   class_mode='categorical',\n",
    "                                                   shuffle=True,\n",
    "                                                   subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {value: key for key, value in train_generator.class_indices.items()}\n",
    "\n",
    "print(\"Label Mappings for classes present in the training and validation datasets\\n\")\n",
    "for key, value in labels.items():\n",
    "    print(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(arr1,arr2):\n",
    "    print ('\\x1b[6;30;46m'+'Accuracy:'+str(np.round(accuracy_score(arr1, arr2),4))+','+' Precision:'+str(np.round(precision_score(arr1, arr2),4))+','+\n",
    "    ' Recall:'+str(np.round(recall_score(arr1, arr2),4))+','+' F1_score:'+str(np.round(f1_score(arr1, arr2),4)))\n",
    "    return              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95bcdc15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Matplotlib requires numpy>=1.20; you have 1.19.5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m7\u001b[39m)) \n\u001b[1;32m      4\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Project/lib/python3.8/site-packages/matplotlib/__init__.py:227\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parse_version(module\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m parse_version(minver):\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatplotlib requires \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m                               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 227\u001b[0m \u001b[43m_check_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# The decorator ensures this always returns the same handler (and it is only\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# attached once).\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache()\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_handler\u001b[39m():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Project/lib/python3.8/site-packages/matplotlib/__init__.py:223\u001b[0m, in \u001b[0;36m_check_versions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(modname)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parse_version(module\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m parse_version(minver):\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatplotlib requires \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: Matplotlib requires numpy>=1.20; you have 1.19.5"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(9, 7)) \n",
    "idx = 0\n",
    "\n",
    "for i in range(3): \n",
    "    for j in range(3): \n",
    "        label = labels[np.argmax(train_generator[0][1][idx])] \n",
    "        ax[i, j].set_title(f\"{label}\")\n",
    "        ax[i, j].imshow(train_generator[0][0][idx][:, :, :]) \n",
    "        ax[i, j].axis(\"off\") \n",
    "        idx += 1\n",
    "\n",
    "plt.tight_layout() \n",
    "#plt.suptitle(\"Sample Training Images\", fontsize=21) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(train_generator)\n",
    "X=(X-X.mean())/X.std()\n",
    "#X_test, y_test = next(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_test, y_train1, y_test = train_test_split(X, y, test_size=0.10, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train1, y_train1, test_size=0.20, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aafd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPUT_SHAPE=(IMG_WIDTH, IMG_HEIGHT, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-production",
   "metadata": {},
   "source": [
    "# Prediction using different Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-edinburgh",
   "metadata": {},
   "source": [
    "# A. CustomCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "values = initializer(shape=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8897b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "y=[np.argmax(i) for i in y_train]\n",
    "class_weights = compute_class_weight(class_weight = \"balanced\",classes = np.unique(y),y = y)\n",
    "class_weights = dict(zip(np.unique(y), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=IMPUT_SHAPE)\n",
    "\n",
    "#Convolution\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\")(input_data)\n",
    "\n",
    "#Pooling\n",
    "x = MaxPooling2D(pool_size = (4, 4), strides=(4, 4))(x)\n",
    "\n",
    "#Dropout\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "# 2nd Convolution\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\")(x)\n",
    "\n",
    "# 2nd Pooling layer\n",
    "x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "\n",
    "#Dropout\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "#3rd Convolution\n",
    "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "\n",
    "#3rd Pooling Layer\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "#Dropout\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Flatten the layer\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Fully Connected Layers\n",
    "x =Dense(128, activation = 'relu')(x)\n",
    "output = Dense(2, activation = 'softmax')(x)\n",
    "\n",
    "cnn =keras.models.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "# Compile the Neural network\n",
    "cnn.compile(optimizer =Adam(learning_rate=0.0001), loss = 'categorical_crossentropy', \n",
    "            metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "filepath11=\"weights.best_custom_cnn1.hdf5\"\n",
    "checkpoint1 = ModelCheckpoint(filepath11, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "rlrop = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=10)\n",
    "callbacks_list = [checkpoint1,es,rlrop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = cnn.fit(X_train,y_train,epochs = 100,verbose = 1,batch_size=4,\n",
    "                  validation_data =(X_test,y_test),callbacks=callbacks_list,\n",
    "                 class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34786e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history1.history['accuracy'])\n",
    "plt.plot(history1.history['val_accuracy'])\n",
    "plt.title('model accuracy: CustomCNN')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('model loss: CustomCNN')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "from keras.models import load_model\n",
    "cnn=load_model('weights.best_custom_cnn.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=cnn.predict(X_test)\n",
    "Pred=[np.argmax(i) for i in pred1]\n",
    "Y_test=[np.argmax(i) for i in y_test]\n",
    "report(Y_test,Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns \n",
    "cm = confusion_matrix(Y_test, Pred)\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "sns.heatmap(cmn, annot=True, fmt='.2f')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('0: COVID positive, 1: COVID negative')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d16d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import save_model\n",
    "save_model(cnn, \"customCNN.h5\")\n",
    "# load and evaluate a saved model\n",
    "loaded_model = models.load_model('customCNN.h5')\n",
    "# summarize model.\n",
    "model=loaded_model\n",
    "train_pred_p=model.predict(X_train)\n",
    "train_pred = np.argmax(train_pred_p, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb6a99",
   "metadata": {},
   "source": [
    "# B. MobileNetV2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13370d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
    "\n",
    "def make_mobilenet_model(image_size, num_classes):\n",
    "    \n",
    "    input_shape = image_size \n",
    "    \n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n",
    "                                                   include_top=False, # Do not include the dense prediction layer\n",
    "                                                   weights=\"imagenet\") # Load imageNet parameters\n",
    "    \n",
    "    # Freeze the base model by making it non trainable\n",
    "    base_model.trainable = False \n",
    "\n",
    "    # create the input layer (Same as the imageNetv2 input size)\n",
    "    inputs = tf.keras.Input(shape=input_shape) \n",
    "    \n",
    "    # apply data augmentation to the inputs\n",
    "    x = inputs\n",
    "     \n",
    "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
    "    x = base_model(x, training=False) \n",
    "    \n",
    "    # Add the new Binary classification layers\n",
    "    # use global avg pooling to summarize the info in each channel\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x) \n",
    "    #include dropout with probability of 0.2 to avoid overfitting\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Flatten()(x)\n",
    "# Fully Connected Layers\n",
    "    x =Dense(128, activation = 'relu')(x)  \n",
    "   \n",
    "    prediction_layer = Dense(2, activation='softmax')\n",
    "    \n",
    "    outputs = prediction_layer(x)\n",
    "    \n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath21=\"weights.best_mobile_net.hdf5\"\n",
    "checkpoint2 = ModelCheckpoint(filepath21, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "es2 = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "rlrop2 = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=10)\n",
    "callbacks_list2 = [checkpoint2,es2,rlrop2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df53e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model using the make_model function\n",
    "image_size = (100,100,3)\n",
    "mobilenet_model = make_mobilenet_model(image_size, num_classes = 2)\n",
    "\n",
    "# Preview the Model Summary\n",
    "mobilenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate = base_learning_rate)\n",
    "initial_epochs = 50\n",
    "batch_size = 64\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "mobilenet_model.compile(optimizer =Adam(learning_rate=0.0001), loss = 'categorical_crossentropy', \n",
    "            metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b869657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2= mobilenet_model.fit(X_train, y_train,batch_size = 4, epochs = 100, validation_data = (X_val, y_val), \n",
    "                                                callbacks=callbacks_list2, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('model accuracy: MobileNetV2')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('model loss:MobileNetV2 ')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966adca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model=load_model('weights.best_mobile_net.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2=mobilenet_model.predict(X_test)\n",
    "Pred=[np.argmax(i) for i in pred2]\n",
    "Y_test=[np.argmax(i) for i in y_test]\n",
    "report(Y_test,Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns \n",
    "cm = confusion_matrix(Y_test, Pred)\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "sns.heatmap(cmn, annot=True, fmt='.2f')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('0: COVID positive, 1: COVID negative')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import save_model\n",
    "save_model(cnn, \"Mobilenetv2.h5\")\n",
    "# load and evaluate a saved model\n",
    "loaded_model = models.load_model('Mobilenetv2.h5')\n",
    "# summarize model.\n",
    "model=loaded_model\n",
    "train_pred_p=model.predict(X_train)\n",
    "train_pred = np.argmax(train_pred_p, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-gazette",
   "metadata": {},
   "source": [
    "# C: DenseNet169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa007f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_densenet_model(image_size, num_classes):\n",
    "    \n",
    "    input_shape = image_size \n",
    "    \n",
    "    base_model = tf.keras.applications.DenseNet169(input_shape=input_shape,\n",
    "                                                   include_top=False, # Do not include the dense prediction layer\n",
    "                                                   weights=\"imagenet\") # Load imageNet parameters\n",
    "    \n",
    "    # Freeze the base model by making it non trainable\n",
    "    base_model.trainable = False \n",
    "\n",
    "    # create the input layer (Same as the imageNetv2 input size)\n",
    "    inputs = tf.keras.Input(shape=input_shape) \n",
    "    \n",
    "    # apply data augmentation to the inputs\n",
    "    x = inputs\n",
    "     \n",
    "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
    "    x = base_model(x, training=False) \n",
    "    \n",
    "    # Add the new Binary classification layers\n",
    "    # use global avg pooling to summarize the info in each channel\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x) \n",
    "    #include dropout with probability of 0.2 to avoid overfitting\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Flatten()(x)\n",
    "    # Fully Connected Layers\n",
    "    x =Dense(128, activation = 'relu')(x)  \n",
    "   \n",
    "    prediction_layer = Dense(2, activation='softmax')\n",
    "    \n",
    "    outputs = prediction_layer(x)\n",
    "    \n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98075ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (100,100,3)\n",
    "densenet_model = make_densenet_model(image_size, num_classes = 2)\n",
    "\n",
    "# Preview the Model Summary\n",
    "densenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a24c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath31=\"weights.best_densenet169.hdf5\"\n",
    "checkpoint3 = ModelCheckpoint(filepath31, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "es3 = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "rlrop3 = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=10)\n",
    "callbacks_list3 = [checkpoint3,es3,rlrop3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_model.compile(optimizer =Adam(learning_rate=0.0001), loss = 'categorical_crossentropy', \n",
    "            metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = densenet_model.fit(X_train, y_train,epochs = 100,verbose = 1,batch_size=2,validation_data =(X_val,y_val)\n",
    "                    ,callbacks=callbacks_list3,class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history3.history['accuracy'])\n",
    "plt.plot(history3.history['val_accuracy'])\n",
    "plt.title('model accuracy: DenseNet169')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history3.history['loss'])\n",
    "plt.plot(history3.history['val_loss'])\n",
    "plt.title('model loss: DenseNet169')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2409871",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('weights.best_densenet169.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3=model.predict(X_test)\n",
    "Pred=[np.argmax(i) for i in pred3]\n",
    "Y_test=[np.argmax(i) for i in y_test]\n",
    "report(Y_test,Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f6d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns \n",
    "cm = confusion_matrix(Y_test, Pred)\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "sns.heatmap(cmn, annot=True, fmt='.2f')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('0: COVID positive, 1: COVID negative')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import save_model\n",
    "save_model(cnn, \"DenseNet169.h5\")\n",
    "# load and evaluate a saved model\n",
    "loaded_model = models.load_model('DenseNet169.h5')\n",
    "# summarize model.\n",
    "model=loaded_model\n",
    "train_pred_p=model.predict(X_train)\n",
    "train_pred = np.argmax(train_pred_p, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc4440",
   "metadata": {},
   "source": [
    "# D: ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b34ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resnet_model(image_size, num_classes):\n",
    "    \n",
    "    input_shape = image_size \n",
    "    \n",
    "    base_model = tf.keras.applications.ResNet50(input_shape=input_shape,\n",
    "                                                   include_top=False, # Do not include the dense prediction layer\n",
    "                                                   weights=\"imagenet\") # Load imageNet parameters\n",
    "    \n",
    "    # Freeze the base model by making it non trainable\n",
    "    base_model.trainable = False \n",
    "\n",
    "    # create the input layer (Same as the imageNetv2 input size)\n",
    "    inputs = tf.keras.Input(shape=input_shape) \n",
    "    \n",
    "    # apply data augmentation to the inputs\n",
    "    x = inputs\n",
    "     \n",
    "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
    "    x = base_model(x, training=False) \n",
    "    \n",
    "    # Add the new Binary classification layers\n",
    "    # use global avg pooling to summarize the info in each channel\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x) \n",
    "    #include dropout with probability of 0.2 to avoid overfitting\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Flatten()(x)\n",
    "    # Fully Connected Layers\n",
    "    x =Dense(128, activation = 'relu')(x)  \n",
    "   \n",
    "    prediction_layer = Dense(2, activation='softmax')\n",
    "    \n",
    "    outputs = prediction_layer(x)\n",
    "    \n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (100,100,3)\n",
    "resnet_model = make_resnet_model(image_size, num_classes = 2)\n",
    "\n",
    "# Preview the Model Summary\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath51=\"weights.best_ResNet50.hdf5\"\n",
    "checkpoint4 = ModelCheckpoint(filepath51, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "es4 = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "rlrop4 = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=10)\n",
    "callbacks_list4 = [checkpoint4,es4,rlrop4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate = base_learning_rate)\n",
    "initial_epochs = 50\n",
    "batch_size = 64\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "resnet_model.compile(optimizer =Adam(learning_rate=0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8738dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = resnet_model.fit(X_train, y_train,epochs = 100,verbose = 1,batch_size=2,validation_data =(X_val,y_val)\n",
    "                    ,callbacks=callbacks_list4,class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981943db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history4.history['accuracy'])\n",
    "plt.plot(history4.history['val_accuracy'])\n",
    "plt.title('model accuracy: ResNet50')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history4.history['loss'])\n",
    "plt.plot(history4.history['val_loss'])\n",
    "plt.title('model loss: ResNet50')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('weights.best_ResNet50.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c4632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4=model.predict(X_test)\n",
    "Pred=[np.argmax(i) for i in pred4]\n",
    "Y_test=[np.argmax(i) for i in y_test]\n",
    "report(Y_test,Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c518fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns \n",
    "cm = confusion_matrix(Y_test, Pred)\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "sns.heatmap(cmn, annot=True, fmt='.2f')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('0: COVID positive, 1: COVID negative')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3931af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import save_model\n",
    "save_model(cnn, \"ResNet50.h5\")\n",
    "# load and evaluate a saved model\n",
    "loaded_model = models.load_model('ResNet50.h5')\n",
    "# summarize model.\n",
    "model=loaded_model\n",
    "train_pred_p=model.predict(X_train)\n",
    "train_pred = np.argmax(train_pred_p, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history1.history['accuracy'])\n",
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history3.history['accuracy'])\n",
    "plt.plot(history4.history['accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['CustomCNN', 'MobileNetV2','DenseNet169','ResNet50'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history3.history['loss'])\n",
    "plt.plot(history4.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['CustomCNN', 'MobileNetV2','DenseNet169','ResNet50'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc7ec8",
   "metadata": {},
   "source": [
    "# ENSEMBLE\n",
    "\n",
    "To create an ensemble of the four models, we will stack their predictions and use Microsoft FLAML AutoML to find an optimal combiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data=np.concatenate([pred1,pred2,pred3,pred4], axis=1)\n",
    "y_data= np.argmax(y_test, axis=1)\n",
    "X_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "automl = AutoML()\n",
    "# Specify automl goal and constraint\n",
    "automl_settings = {\n",
    "    \"time_budget\": 100,  # total running time in seconds\n",
    "    \"task\": 'classification',  # task type\n",
    "    \"seed\": 24545678,  # random seed\n",
    "    \"metric\" : 'accuracy'}\n",
    "\n",
    "automl.fit(X_train=X_data, y_train=y_data, **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred5=automl.predict(X_data)\n",
    "report(y_data,pred5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaacb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns \n",
    "cm = confusion_matrix(y_data, pred5)\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "sns.heatmap(cmn, annot=True, fmt='.2f')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('0: COVID positive, 1: COVID negative')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from functools import reduce\n",
    "from itertools import product\n",
    "from IPython.display import Image\n",
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "z=[ [0.94,0.97,0.98,0.98,0.99],[0.91,0.95,0.98,0.98,0.99], [0.97,0.98,0.98,0.98,0.99],[0.94,0.97,0.98,0.98,0.99]]\n",
    "x=['<b>CustomCNN</b>', '<b>MobileNetV1</b>', '<b>DenseNet169</b>', '<b>ResNet50</b>', '<b>Ensemble</b>']\n",
    "y=['<b>F1_score</b>', '<b>Recall</b>', '<b>Precision</b>','<b>Accuracy</b>']\n",
    "\n",
    "def get_anno_text(z_value):\n",
    "    annotations=[]\n",
    "    a, b = len(z_value), len(z_value[0])\n",
    "    flat_z = reduce(lambda x,y: x+y, z_value) # z_value.flat if you deal with numpy\n",
    "    coords = product(range(a), range(b))\n",
    "    for pos, elem in zip(coords, flat_z):\n",
    "        annotations.append({'font': {'color': 'black'},\n",
    "                    'showarrow': False,\n",
    "                    'text': str(elem),\n",
    "                    'x': pos[1],\n",
    "                    'y': pos[0],\n",
    "                        'font.size':22   })\n",
    "    return annotations\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                   z=z,\n",
    "                   x=x,\n",
    "                   y=y,\n",
    "                   hoverongaps = True, colorscale ='turbid',\n",
    "    opacity=0.6,colorbar=dict(tickfont=dict(size=20)) ))#matter#\n",
    "\n",
    "fig.update_layout(title={'text': \"\",\n",
    "        'y':0.8,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "           plot_bgcolor='rgba(0,0,0,0)',       \n",
    "    annotations = get_anno_text(z),\n",
    "                 width=1000,\n",
    "height=400,xaxis={'side': 'top'},margin=dict(l=20, r=20, t=20, b=20))\n",
    "\n",
    "fig.update_xaxes(tickfont = dict(size=24),linewidth=0.1, linecolor='black',\n",
    "    \n",
    "                 mirror=True)\n",
    "fig.update_yaxes(tickfont = dict(size=24),linewidth=0.1, linecolor='black',\n",
    "        \n",
    "                 mirror=True)\n",
    "fig.write_image(\"table2b.png\",engine=\"kaleido\")\n",
    "#plt.savefig(\"table2a.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "fig.show()\n",
    "Image('table2b.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54077d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
